---
title: "Untitled"
author: "Pierre Camilleri"
date: "28 mai 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries
library(tidyverse)
#library(tricky)
library(lubridate)
library(assertthat)
library(mongolite)
library(broom)
library(randomForest)
library(MLmetrics)
library(tibbletime)
library(PRROC)
library(broom)
library(keras)
library(reshape2)
library(rprojroot)

source(find_rstudio_root_file('tools','interface','connect_to_database.R'))
source(find_rstudio_root_file('tools','objective','objective_default_or_failure.R'))
source(find_rstudio_root_file('tools','objective','set_objective.R'))
source(find_rstudio_root_file('tools','split','generator.R'))
source(find_rstudio_root_file('tools','data_prep','monthly_to_yearly.R'))
source(find_rstudio_root_file('tools','data_prep','df_to_RNN_input.R'))
source(find_rstudio_root_file('tools','data_prep','normalize_df.R'))
source(find_rstudio_root_file('tools','data_prep','feature_engineering_RNN.R'))
source(find_rstudio_root_file('tools','utilities','elapsed_months.R'))
source(find_rstudio_root_file('tools','post_analysis','prepare_for_export.R'))
source(find_rstudio_root_file('tools','post_analysis','export.R'))
source(find_rstudio_root_file('tools','models','model_tune.R'))
source(find_rstudio_root_file('tools','models','model_cv_eval.R'))
source(find_rstudio_root_file('tools','models','model_predict_random_forest.R'))


```

## Definition periode actuelle
```{r}
actual_period <- as.Date("2018-05-01")
```

## Recuperation des données dans MongoDB
```{r}
raw_data <- connect_to_database('Features','1806', algo = 'algo2') 
```
```{r}
raw_data <- feature_engineering_RNN(raw_data)
```

## Definition de l'objectif
```{r}
raw_data <- raw_data %>% 
  objective_default_or_failure(n_months = 3, threshold = 1, lookback = 18) %>%
  set_objective('default')
```

## Split train test 

Avec suréchantillonnage. Les suréchantillons seront supprimés à la main dans les échantillons d'entraînement pour s'assurer une évaluation sur le même échantillon de validation croisé. 
```{r}

set.seed(9496)
all_sirets <- raw_data %>% 
  group_by(siret) %>% 
  summarize()

train_siret <- all_sirets  %>%
  sample_frac(0.6)

validation_siret <- all_sirets %>% 
  anti_join(train_siret, by = 'siret') %>% 
  sample_frac(0.5)

test_siret <- all_sirets %>% 
  anti_join(train_siret, by = 'siret') %>% 
  anti_join(validation_siret, by = 'siret')


# Normalisation 

aux_scale <- raw_data %>%
  semi_join(train_siret, by = 'siret') %>%
  normalize_df()

aux2_scale <- raw_data %>% 
  normalize_df(means = aux_scale[['means']], stds = aux_scale[['stds']])

scaled_data <- aux2_scale[['data']]

rm(aux_scale) 
rm(aux2_scale)

```

```{r}

 var_monthly <-  c('effectif',
                    'cotisation',
                    'apart_heures_consommees',
                    'apart_motif_recours',
                    'etat_proc_collective',
                    'montant_part_ouvriere',
                    'montant_part_patronale',
                    'effectif_entreprise',
                    'apart_entreprise',
                    'delai',
                    'duree_delai',
                    'montant_echeancier'
                    )

var_yearly <- c('poids_frng',
                    'taux_marge',
                    'delai_fournisseur',
                    'dette_fiscale',
                    'financier_court_terme',
                    'frais_financier',
                    'age')

var_other <- c('activite_saisonniere',
               'indice_monoactivite',
               'productif', 
               'nbr_etablissements_connus')

lookback_monthly_data = 12

lookback_yearly_data = 3

seed2 <- 10011

aux_gen_fun <- function(sirets, batch_size) {
  return(
  generator(
    data = scaled_data,
    var_yearly = var_yearly,
    lookback_yearly_data = lookback_yearly_data,
    var_monthly = var_monthly,
    lookback_monthly_data = lookback_monthly_data,
    var_other = var_other,
    sirets = unlist(sirets),
    date_inf = '2014-01-01',
    date_sup = '2016-01-01',
    seed = seed2,
    batch_size = batch_size
  )
  )
}


train_gen <- aux_gen_fun(train_siret,128)
validation_gen = aux_gen_fun(validation_siret,64)
test_gen <- aux_gen_fun(test_siret,64)
```

## Model
### RNN
model
```{r}

monthly_input <- layer_input(shape = list(lookback_monthly_data,length(var_monthly)), name = "monthly_input")

monthly_NN <- monthly_input %>% 
  layer_gru(units = 32, recurrent_dropout = 0.5, dropout = 0.1)

yearly_input <- layer_input(shape = list(lookback_yearly_data,length(var_yearly)), name = "yearly_input")

yearly_NN <- yearly_input %>% 
  layer_simple_rnn(units = 32, recurrent_dropout = 0.5, dropout = 0.1)


ape_input <- layer_input(shape = list(1), name = "ape_input")

ape_NN <- ape_input %>% 
  layer_embedding(input_dim = 522, output_dim = 16,name = 'embedding') %>% 
  layer_flatten() 

other_input <- layer_input(shape = list(length(var_other)), name = "other_input")

other_NN <- other_input %>%
  layer_dense(units = 16, activation = 'relu')


concatenated <- layer_concatenate(list(yearly_NN,monthly_NN,ape_NN, other_NN))

output <-  concatenated %>% 
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 64,activation = 'relu') %>%
  layer_dense(units = 32, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'sigmoid')

model <- keras_model(list(
  monthly_input,
  yearly_input,
  ape_input,
  other_input),
            output)


```

Callbacks
```{r}
callback_list <- list(
  callback_early_stopping(
    monitor = "acc",
    patience = 3
  ), 
  callback_model_checkpoint(
    filepath = find_rstudio_root_file('pipes','RNNs','RNN.h5'),
    monitor = "val_loss",
    save_best_only = TRUE),
  callback_tensorboard(
    log_dir = 'my_log_dir',
    histogram_freq = 0,
    embeddings_freq = 1
  )
  )

tensorboard(find_rstudio_root_file('pipes','my_log_dir'))
```


```{r}
model %>% compile(
 # optimizer = optimizer_rmsprop(lr = 0.0001),
  optimizer = optimizer_adam(lr = 0.0001),
  loss = "binary_crossentropy",
  metrics = c('acc')
)

```

```{r}

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 40,
  epochs = 50,
  validation_data = validation_gen,
  validation_steps = 15,
  verbose = TRUE,
  callbacks  = callback_list
)
```

