---
title: "Untitled"
author: "Pierre Camilleri"
date: "28 mai 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries
library(tidyverse)
library(tricky)
library(lubridate)
library(assertthat)
library(mongolite)
library(broom)
library(randomForest)
library(MLmetrics)
library(tibbletime)
library(PRROC)
library(broom)
library(keras)
library(reshape2)
library(rprojroot)

source(find_rstudio_root_file('tools','interface','connect_to_database.R'))
source(find_rstudio_root_file('tools','objective','objective_default_or_failure.R'))
source(find_rstudio_root_file('tools','objective','set_objective.R'))
source(find_rstudio_root_file('tools','split','generator.R'))
source(find_rstudio_root_file('tools','data_prep','df_to_RNN_input.R'))
source(find_rstudio_root_file('tools','data_prep','normalize_df.R'))
source(find_rstudio_root_file('tools','utilities','elapsed_months.R'))
source(find_rstudio_root_file('tools','post_analysis','prepare_for_export.R'))
source(find_rstudio_root_file('tools','post_analysis','export.R'))


```

## Definition periode actuelle
```{r}
actual_period <- as.Date("2018-04-01")
```

## Recuperation des données dans MongoDB
```{r}
raw_data <- connect_to_database('Features')
```

## Definition de l'objectif
```{r}
raw_data <- raw_data %>% 
  objective_default_or_failure(n_months = 3, threshold = 1, lookback = 24) %>%
  set_objective('default')
```

## Split train test 

Avec suréchantillonnage. Les suréchantillons seront supprimés à la main dans les échantillons d'entraînement pour s'assurer une évaluation sur le même échantillon de validation croisé. 
```{r}
#generateur beaucoup TROP lent FIX ME

set.seed(9496)
all_sirets <- raw_data %>% 
  group_by(siret) %>% 
  summarize()

train_siret <- all_sirets  %>%
  sample_frac(0.6)

validation_siret <- all_sirets %>% 
  anti_join(train_siret, by = 'siret') %>% 
  sample_frac(0.5)

test_siret <- all_sirets %>% 
  anti_join(train_siret, by = 'siret') %>% 
  anti_join(validation_siret, by = 'siret')


# Normalisation 

aux_scale <- raw_data %>%
  semi_join(train_siret, by = 'siret') %>%
  normalize_df()

aux2_scale <- raw_data %>% 
  normalize_df(means = aux_scale[['means']], stds = aux_scale[['stds']])

scaled_data <- aux2_scale[['data']]


```

```{r}

seed2 <- 10011
train_gen <-
  generator(
  scaled_data,
  3,
  12,
  unlist(train_siret),
  '2014-01-01',
  '2017-01-01',
  seed2,
  batch_size = 128
  )

validation_gen <-
  generator(
  scaled_data,
  3,
  12,
  unlist(validation_siret),
  '2014-01-01',
  '2017-01-01',
  seed2,
  batch_size = 64
)

test_gen <-
  generator(
  scaled_data,
  3,
  12,
  unlist(test_siret),
  '2014-01-01',
  '2017-01-01',
  seed2,
  batch_size = 64
)

```

## Model
### Reference

```{r}
# df_for_glm <-  as.data.frame(cbind(train_aux[[1]], train_aux[[2]]))
# names(df_for_glm) <- c('effectif',
#                     'log_cotisationdue',
#                     'log_ratio', 
#                     'outcome')
# 
# df_for_glm <- df_for_glm %>% 
#   mutate(outcome = as.factor(outcome))
# 
# model_simple <- glm(formula = 'outcome ~ effectif + log_cotisationdue + log_ratio',data =  df_for_glm ,family = binomial())
# 
# round(predict(object = model_simple, newdata = df_for_glm %>% select(-outcome), type = 'response'))
# 
# sum(.Last.value == df_for_glm$outcome) / length(df_for_glm$outcome)
```

### RNN
first try

model
```{r}
# monModel <- keras_model_sequential() %>%
#   #layer_simple_rnn(units = 3, input_shape = c(12,3)) %>%
#   layer_dense(units = 24, activation = 'relu', input_shape = c(3)) %>% 
#   layer_dense(units = 1, activation = 'softmax')
# 
# # model %>% compile(
# #   optimizer = optimizer_rmsprop(),
# #   loss = "binary_crossentropy",
# #   metrics = c('acc')
# # )


```

```{r}

# monthly_input <- layer_input(shape = c(12,3))
# 
# monthly_NN <- monthly_input #%>% 
#   # layer_simple_rnn(units = 32) %>%
#   #layer_dense(units = 36, activation = 'relu') %>% 
#   #layer_dense(units = 18,activation = 'relu')
# 
# output <-  monthly_NN %>% 
#   layer_simple_rnn(units = 16, activation = 'relu') %>% 
#   layer_dense(units = 1, activation = 'sigmoid')
# 
# monModel <- keras_model(monthly_input, #, other_input),
#             output)

```

entraînement

```{r}
# tensorboard('my_log_dir')
# # callbacks = list(
# #   callback_tensorboard(
# #     log_dir = "my_log_dir",
# #     histogram_freq = 1
# #   )  
# # )
# 
# monModel %>% compile(
#   optimizer = optimizer_rmsprop(lr = 0.001),
#   loss = "binary_crossentropy",
#   metrics = c('acc')
# )
# 
# train_aux <- train_gen();
# 
# 
# history <- monModel %>% fit(
#    x = train_aux[[1]],
#    y = train_aux[[2]],
#    batch_size = 1,
#    epochs = 100,
#    verbose = TRUE #,
#   #callbacks = callbacks
# )
#  # history <- model %>% fit_generator(
#  #   train_gen,
#  #  steps_per_epoch = 20,
#  #   epochs = 5,
#  #   validation_data = validation_gen,
#  #   validation_steps = 5,
#  #   verbose = TRUE,
#  #  callbacks = callbacks
# # )
```
second try: incorporating yearly data
model
```{r}

monthly_input <- layer_input(shape = list(12,3), name = "monthly_input")

monthly_NN <- monthly_input %>% 
  layer_gru(units = 32, recurrent_dropout = 0.5, dropout = 0.1)

yearly_input <- layer_input(shape = list(3,6), name = "yearly_input")

yearly_NN <- yearly_input %>% 
  layer_simple_rnn(units = 32, recurrent_dropout = 0.5, dropout = 0.1)


other_input <- layer_input(shape = list(1), name = "other_input")

other_NN <- other_input %>% 
  layer_embedding(input_dim = 514, output_dim = 16,name = 'embedding') %>% 
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu')

concatenated <- layer_concatenate(list(yearly_NN,monthly_NN,other_NN))

output <-  concatenated %>% 
  layer_dense(units = 32, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'sigmoid')

model <- keras_model(list(
  monthly_input,
  yearly_input,
  other_input),
            output)


```

```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.0001),
  loss = "binary_crossentropy",
  metrics = c('acc')
)

```

```{r}

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 40,
  epochs = 6,
  validation_data = validation_gen,
  validation_steps = 15,
  verbose = TRUE
)
```

